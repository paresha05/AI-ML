{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVE30EMThWPj"
      },
      "outputs": [],
      "source": [
        "\"\"\"!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install tokenizer\n",
        "!pip install seqeval\n",
        "\"\"\"\n",
        "#either you can use above statement or use single command\n",
        "# Install\n",
        "!pip install transformers datasets tokenizers seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "IkF5laABlwL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003 = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "BOSMUGPtnkH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003"
      ],
      "metadata": {
        "id": "kmNlLA8SofOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"]"
      ],
      "metadata": {
        "id": "7Q5bDAubptQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"][0]"
      ],
      "metadata": {
        "id": "a39FQ94Kp0QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"]"
      ],
      "metadata": {
        "id": "iR8tTo6iqa4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"].features['ner_tags']"
      ],
      "metadata": {
        "id": "I75eeKXcp4qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"].description"
      ],
      "metadata": {
        "id": "WEwLrLrxqQRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "HmxRWM5Orijk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'][0]"
      ],
      "metadata": {
        "id": "StnO3YtTxxCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"].features['ner_tags']"
      ],
      "metadata": {
        "id": "hQ5jjxtWx0HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = conll2003['train'][0]"
      ],
      "metadata": {
        "id": "iyitpGdMyF0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text"
      ],
      "metadata": {
        "id": "gdmSPwNxyUmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text[\"tokens\"]"
      ],
      "metadata": {
        "id": "k4rJLE_WykFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True)"
      ],
      "metadata": {
        "id": "te0BldE9yWXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input[\"input_ids\"]"
      ],
      "metadata": {
        "id": "iBePj4qVypbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input"
      ],
      "metadata": {
        "id": "x394QYEBy3P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "sbCZhDpEyq1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "OAUZb2t3y9Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = tokenized_input.word_ids()\n",
        "\n",
        "print(word_ids)"
      ],
      "metadata": {
        "id": "GyOSDn4py-L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text[\"ner_tags\"]"
      ],
      "metadata": {
        "id": "cfqg5K6z0sPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(example_text[\"ner_tags\"]):\n",
        "  print(i,label)"
      ],
      "metadata": {
        "id": "N02NWExd0zrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "\n",
        "    #tokeinze ids\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # word_ids() => Return a list mapping the tokens\n",
        "        # to their actual word in the initial sentence.\n",
        "        # It Returns a list indicating the word corresponding to each token.\n",
        "\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
        "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # set â€“100 as the label for these special tokens\n",
        "                label_ids.append(-100)\n",
        "\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # if current word_idx is != prev then its the most regular case\n",
        "                # and add the corresponding token\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # to take care of sub-words which have the same word_idx\n",
        "                # set -100 as well for them, but only if label_all_tokens == False\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "                # mask the subword representations after the first subword\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "3ubJqVH1zN9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"][4:5]"
      ],
      "metadata": {
        "id": "nhXC-akx1p6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q=tokenize_and_align_labels(conll2003[\"train\"][4:5])"
      ],
      "metadata": {
        "id": "CtIPEziA1szx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
        "    print(f\"{token:_<40} {label}\")"
      ],
      "metadata": {
        "id": "LHM4esZX2COy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Applying on entire data\n",
        "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "-fEdnsKH2LyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "TNpnZMfF29im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\",num_labels=9)"
      ],
      "metadata": {
        "id": "l-eRkzw53Eoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "lMiMj7mK3x0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "\"test-ner\",\n",
        "evaluation_strategy = \"epoch\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=16,\n",
        "per_device_eval_batch_size=16,\n",
        "num_train_epochs=1,\n",
        "weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "pbVuoJjP4CUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "KZh154g24cDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer(\n",
        "   model,\n",
        "   args,\n",
        "   train_dataset=tokenized_datasets[\"train\"],\n",
        "   eval_dataset=tokenized_datasets[\"validation\"],\n",
        "   data_collator=data_collator,\n",
        "   tokenizer=tokenizer,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "OVoXke9U4jLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric=datasets.load_metric(\"seqeval\")"
      ],
      "metadata": {
        "id": "eRDqIBmz6F1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=conll2003['train'][0]"
      ],
      "metadata": {
        "id": "VftjL1R05U-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "label_list"
      ],
      "metadata": {
        "id": "Ltu1T51R6R06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in example[\"ner_tags\"]:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "w-38wIa86WCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " example"
      ],
      "metadata": {
        "id": "5IQV9rfQ6o8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "labels"
      ],
      "metadata": {
        "id": "Gni6IvWC6XTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric.compute(predictions=[labels],references=[labels])"
      ],
      "metadata": {
        "id": "UD-rI-s-6jgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    pred_logits, labels = eval_preds\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "    # the logits and the probabilities are in the same order,\n",
        "    # so we donâ€™t need to apply the softmax\n",
        "\n",
        "    # We remove all the values where the label is -100\n",
        "    predictions = [\n",
        "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "       for prediction, label in zip(pred_logits, labels)\n",
        "   ]\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "          \"precision\": results[\"overall_precision\"],\n",
        "          \"recall\": results[\"overall_recall\"],\n",
        "          \"f1\": results[\"overall_f1\"],\n",
        "          \"accuracy\": results[\"overall_accuracy\"],\n",
        "  }"
      ],
      "metadata": {
        "id": "39jv0s-a7Kte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator=DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "uwVulRS-8JdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=Trainer(\n",
        "   model,\n",
        "   args,\n",
        "   train_dataset=tokenized_datasets[\"train\"],\n",
        "   eval_dataset=tokenized_datasets[\"validation\"],\n",
        "   data_collator=data_collator,\n",
        "   tokenizer=tokenizer,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "2XSqn0Sb7qtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MwRQ2KPr8ThI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"ner_model\")"
      ],
      "metadata": {
        "id": "6vuTSmBu8V6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "u0tA7MCe9MVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list"
      ],
      "metadata": {
        "id": "A57V_ELI9XVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    str(i): label for i,label in enumerate(label_list)\n",
        "}"
      ],
      "metadata": {
        "id": "O54dK99e-D4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "id": "F_sNX73b-HT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\n",
        "    label: str(i) for i,label in enumerate(label_list)\n",
        "}"
      ],
      "metadata": {
        "id": "8c6J4gAD-JnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id"
      ],
      "metadata": {
        "id": "aAbha6hd-MHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "V0LfS320-Pfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=json.load(open(\"/content/ner_model/config.json\"))"
      ],
      "metadata": {
        "id": "hC0wBuQb-Rgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "M6YoLMEq-g85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"id2label\"] = id2label\n"
      ],
      "metadata": {
        "id": "oqWpyHoh-cKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"label2id\"] = label2id"
      ],
      "metadata": {
        "id": "TBoL387O-cNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(config,open(\"/content/ner_model/config.json\",\"w\"))"
      ],
      "metadata": {
        "id": "nhT4qrDP-cS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tuned=AutoModelForTokenClassification.from_pretrained(\"ner_model\")"
      ],
      "metadata": {
        "id": "U1oEoLWS-cUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tuned"
      ],
      "metadata": {
        "id": "Ko-y7l0C_F73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer pipeline"
      ],
      "metadata": {
        "id": "fQLAZsxW9jT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "Ay6rKFif9j67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline=pipeline(\"ner\",model=model_fine_tuned,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "d7kMV1wQ9na-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline"
      ],
      "metadata": {
        "id": "uoqS99LO_fy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"sudhanshu kumar is a foundar of iNeuron\""
      ],
      "metadata": {
        "id": "tYbT5QgDANPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline(example)"
      ],
      "metadata": {
        "id": "8UqJsroVAXnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"sunny is a founder of microsoft\""
      ],
      "metadata": {
        "id": "436NtdA-ArZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline(example)"
      ],
      "metadata": {
        "id": "wkpVk_g3Ab4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"apple launch mobile while eating apple which taste like orange\"\n"
      ],
      "metadata": {
        "id": "0h-wIi2hAzSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline(example)"
      ],
      "metadata": {
        "id": "N_v7CVYsBFoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"vikas is working ai engineer in google\""
      ],
      "metadata": {
        "id": "ISyq3QOwBHQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline(example)"
      ],
      "metadata": {
        "id": "g36EN5IBBSLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"apple founder loves eating apple\"\n"
      ],
      "metadata": {
        "id": "fscsPD8eBTjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline(example)"
      ],
      "metadata": {
        "id": "UXKpdqM4BdL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"Microsoft Windows created their software by idea that came from the window of the house\"\n"
      ],
      "metadata": {
        "id": "mpKOmQtMBeVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline(example)"
      ],
      "metadata": {
        "id": "DokBMRjRCAP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5edahwCOCBRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
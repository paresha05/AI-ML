{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "id": "6Dx2eOTpRhdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "g5_2QzEbRjhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl peft accelerate datasets bitsandbytes"
      ],
      "metadata": {
        "id": "p6YvL4KER0kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "gHpuJS5DR9kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "0KgAzmmFTI6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "c9BBtQ3kTLLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset,Dataset"
      ],
      "metadata": {
        "id": "cTd3PH1BTnY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-MJ-7_Gfy66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "C-pgdkOLULZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "6sk0aOsMUOsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "dUTuX_RyUTB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model"
      ],
      "metadata": {
        "id": "ZCyAYoFcUUKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments"
      ],
      "metadata": {
        "id": "KBNuqX0xUXhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config={\n",
        "\n",
        "            \"model_ckpt\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "            \"load_in_4bit\": True,\n",
        "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"trust_remote_code\": True,\n",
        "\n",
        "            \"use_lora\": True,\n",
        "            \"r\": 16,\n",
        "            \"lora_alpha\": 16,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"bias\": \"none\",\n",
        "            \"task_type\": \"CAUSAL_LM\",\n",
        "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "\n",
        "            \"output_dir\": \"sft-tiny-chatbot\",\n",
        "            \"per_device_train_batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 1,\n",
        "            \"optim\": \"paged_adamw_32bit\",\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lr_scheduler_type\": \"cosine\",\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_steps\": 100,\n",
        "            \"num_train_epochs\": 1,\n",
        "            \"max_steps\": 250,\n",
        "            \"fp16\": True,\n",
        "            \"push_to_hub\": True,\n",
        "            \"train_cln_name\": \"text\",\n",
        "            \"packing\": False,\n",
        "            \"max_seq_length\": 512,\n",
        "            \"neftune_noise_alpha\": 5\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "i_yZdf1RUc-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainSFT:\n",
        "\n",
        "  def __init__(self,data,config):\n",
        "    self.data=data\n",
        "    self.config=config\n",
        "\n",
        "  def prepare_lora_model(self):\n",
        "\n",
        "    self.lora_config= LoraConfig(r=self.config[\"r\"],\n",
        "                                    lora_alpha=self.config[\"lora_alpha\"],\n",
        "                                    lora_dropout=self.config[\"lora_dropout\"],\n",
        "                                    bias=self.config[\"bias\"],\n",
        "                                    task_type=self.config[\"task_type\"],\n",
        "                                    target_modules=self.config[\"target_modules\"])\n",
        "\n",
        "    self.model= get_peft_model(self.model,self.lora_config)\n",
        "\n",
        "  def load_model_tokenizer(self):\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"])\n",
        "    self.model.config.use_cache=False\n",
        "    self.model.config.pretraining_tp=1\n",
        "    self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "    if self.config[\"use_lora\"]:\n",
        "      self.prepare_lora_model()\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"model_ckpt\"])\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "  def set_training_args(self):\n",
        "\n",
        "\n",
        "    return TrainingArguments(\n",
        "                                    output_dir=self.config[\"output_dir\"],\n",
        "                                    per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
        "                                    gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
        "                                    optim=self.config[\"optim\"],\n",
        "                                    learning_rate=self.config[\"learning_rate\"],\n",
        "                                    lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
        "                                    save_strategy=self.config[\"save_strategy\"],\n",
        "                                    logging_steps=self.config[\"logging_steps\"],\n",
        "                                    num_train_epochs=self.config[\"num_train_epochs\"],\n",
        "                                    max_steps=self.config[\"max_steps\"],\n",
        "                                    fp16=self.config[\"fp16\"],\n",
        "                                    push_to_hub=self.config[\"push_to_hub\"],\n",
        "                                    neftune_noise_alpha=self.config[\"neftune_noise_alpha\"]\n",
        "                                )\n",
        "  def create_trainer(self):\n",
        "\n",
        "        self.load_model_tokenizer()\n",
        "        if self.config[\"use_lora\"]:\n",
        "            print(self.model.print_trainable_parameters())\n",
        "            self.trainer = SFTTrainer(\n",
        "                                    model=self.model,\n",
        "                                    train_dataset=self.data,\n",
        "                                    peft_config=self.lora_config,\n",
        "                                    dataset_text_field=self.config[\"train_cln_name\"],\n",
        "                                    args=self.set_training_args(),\n",
        "                                    tokenizer=self.tokenizer,\n",
        "                                    packing=self.config[\"packing\"],\n",
        "                                    max_seq_length=self.config[\"max_seq_length\"]\n",
        "                                )\n",
        "        else:\n",
        "            self.trainer = SFTTrainer(\n",
        "                                    model=self.model,\n",
        "                                    train_dataset=self.data,\n",
        "                                    dataset_text_field=self.config[\"train_cln_name\"],\n",
        "                                    args=self.set_training_args(),\n",
        "                                    tokenizer=self.tokenizer,\n",
        "                                    packing=self.config[\"packing\"],\n",
        "                                    max_seq_length=self.config[\"max_seq_length\"]\n",
        "                                )\n",
        "\n",
        "  def train_and_save_model(self):\n",
        "    self.create_trainer()\n",
        "    self.trainer.train()\n",
        "    self.trainer.save_model(self.config[\"output_dir\"])\n",
        "    self.tokenizer.save_pretrained(self.config[\"output_dir\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "v7XEvuZpVzfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset,Dataset"
      ],
      "metadata": {
        "id": "Ivle21kXf0xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data():\n",
        "    data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "    data_df = data.to_pandas()\n",
        "    data_df = data_df[:700]\n",
        "    data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" Assistant: \"+ x[\"output\"], axis=1)\n",
        "    data = Dataset.from_pandas(data_df)\n",
        "    return data\n",
        "\n",
        "data = create_data()"
      ],
      "metadata": {
        "id": "BYs4OEGSYu9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sft = TrainSFT(data, sft_config)\n",
        "train_sft.train_and_save_model()"
      ],
      "metadata": {
        "id": "zwLnDH4kY9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preference Alignmemnt- DPO"
      ],
      "metadata": {
        "id": "PmBLrkCZf-66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_config = {\n",
        "            \"model_ckpt\": \"snshrivas10/sft-tiny-chatbot\",\n",
        "            \"load_in_4bit\": True,\n",
        "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"trust_remote_code\": True,\n",
        "            \"use_lora\": True,\n",
        "            \"r\": 8,\n",
        "            \"lora_alpha\": 8,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"bias\": \"none\",\n",
        "            \"task_type\": \"CAUSAL_LM\",\n",
        "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "            \"output_dir\": \"tiny-chatbot-dpo\",\n",
        "            \"per_device_train_batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 1,\n",
        "            \"optim\": \"paged_adamw_32bit\",\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lr_scheduler_type\": \"cosine\",\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_steps\": 100,\n",
        "            \"num_train_epochs\": 1,\n",
        "            \"max_steps\": 250,\n",
        "            \"fp16\": True,\n",
        "            \"push_to_hub\": True,\n",
        "            \"train_cln_name\": \"text\",\n",
        "            \"packing\": False,\n",
        "            \"neftune_noise_alpha\": 5,\n",
        "            \"beta\": 0.1,\n",
        "            \"loss_type\": \"kto_pair\",\n",
        "            \"max_length\": 768,\n",
        "            \"max_prompt_length\": 512,\n",
        "            \"max_target_length\": 256,\n",
        "            \"is_encoder_decoder\": False\n",
        "        }"
      ],
      "metadata": {
        "id": "J-s8eLn3hZ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "import pandas as pd\n",
        "from accelerate import Accelerator\n",
        "from peft import get_peft_model, LoraConfig"
      ],
      "metadata": {
        "id": "qvM003sKgDGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDPO:\n",
        "\n",
        "  def __init__(self,data,config):\n",
        "    self.data=data\n",
        "    self.config=config\n",
        "\n",
        "  def prepare_lora_model(self):\n",
        "\n",
        "        self.lora_config = LoraConfig(\n",
        "                                    r=self.config[\"r\"],\n",
        "                                    lora_alpha=self.config[\"lora_alpha\"],\n",
        "                                    lora_dropout=self.config[\"lora_dropout\"],\n",
        "                                    bias=self.config[\"bias\"],\n",
        "                                    task_type=self.config[\"task_type\"],\n",
        "                                    target_modules=self.config[\"target_modules\"]\n",
        "                                )\n",
        "        self.model = get_peft_model(self.model, self.lora_config)\n",
        "        self.model_ref = get_peft_model(self.model_ref, self.lora_config)\n",
        "\n",
        "  def load_model_tokenizer(self):\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"]\n",
        "                        )\n",
        "\n",
        "        self.model_ref = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"]\n",
        "                        )\n",
        "        self.model.config.use_cache=False\n",
        "        self.model.config.pretraining_tp=1\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "        if self.config[\"use_lora\"]:\n",
        "            self.prepare_lora_model()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"model_ckpt\"])\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "  def set_training_args(self):\n",
        "\n",
        "        return TrainingArguments(\n",
        "                                    output_dir=self.config[\"output_dir\"],\n",
        "                                    per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
        "                                    gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
        "                                    optim=self.config[\"optim\"],\n",
        "                                    learning_rate=self.config[\"learning_rate\"],\n",
        "                                    lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
        "                                    save_strategy=self.config[\"save_strategy\"],\n",
        "                                    logging_steps=self.config[\"logging_steps\"],\n",
        "                                    num_train_epochs=self.config[\"num_train_epochs\"],\n",
        "                                    max_steps=self.config[\"max_steps\"],\n",
        "                                    fp16=self.config[\"fp16\"],\n",
        "                                    push_to_hub=self.config[\"push_to_hub\"],\n",
        "                                    neftune_noise_alpha=self.config[\"neftune_noise_alpha\"]\n",
        "                                )\n",
        "\n",
        "\n",
        "  def create_trainer(self):\n",
        "\n",
        "        self.load_model_tokenizer()\n",
        "        if self.config[\"use_lora\"]:\n",
        "            print(self.model.print_trainable_parameters())\n",
        "            self.trainer = DPOTrainer(\n",
        "                                        self.model,\n",
        "                                        self.model_ref,\n",
        "                                        args=self.set_training_args(),\n",
        "                                        train_dataset=self.data,\n",
        "                                        tokenizer=self.tokenizer,\n",
        "                                        beta=self.config[\"beta\"],\n",
        "                                        loss_type=self.config[\"loss_type\"],\n",
        "                                        max_length=self.config[\"max_length\"],\n",
        "                                        max_prompt_length=self.config[\"max_prompt_length\"],\n",
        "                                        max_target_length=self.config[\"max_target_length\"],\n",
        "                                        is_encoder_decoder=self.config[\"is_encoder_decoder\"]\n",
        "                                    )\n",
        "        else:\n",
        "            self.trainer = DPOTrainer(\n",
        "                                        self.model,\n",
        "                                        self.model_ref,\n",
        "                                        peft_config=self.lora_config,\n",
        "                                        args=self.set_training_args(),\n",
        "                                        train_dataset=self.data,\n",
        "                                        tokenizer=self.tokenizer,\n",
        "                                        beta=self.config[\"beta\"],\n",
        "                                        loss_type=self.config[\"loss_type\"],\n",
        "                                        max_length=self.config[\"max_length\"],\n",
        "                                        max_prompt_length=self.config[\"max_prompt_length\"],\n",
        "                                        max_target_length=self.config[\"max_target_length\"],\n",
        "                                        is_encoder_decoder=self.config[\"is_encoder_decoder\"]\n",
        "                                    )\n",
        "  def train_and_save_model(self):\n",
        "    self.create_trainer()\n",
        "    self.trainer.train()\n",
        "    self.trainer.save_model(self.config[\"output_dir\"])\n",
        "    self.tokenizer.save_pretrained(self.config[\"output_dir\"])\n"
      ],
      "metadata": {
        "id": "_Tv9Zogagpf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data():\n",
        "    df = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\").to_pandas()\n",
        "    df[\"prompt\"] = df[\"chosen\"].apply(lambda x: x.split(\"Assistant: \")[0])\n",
        "    df[\"chosen\"] = df[\"chosen\"].apply(lambda x: \"Assistant: \"+ x.split(\"Assistant: \")[-1])\n",
        "    df[\"rejected\"] = df[\"rejected\"].apply(lambda x: \"Assistant: \" + x.split(\"Assistant: \")[-1])\n",
        "    df = df.sample(1000)\n",
        "    data = Dataset.from_pandas(df)\n",
        "    return data\n",
        "data = create_data()"
      ],
      "metadata": {
        "id": "wnPzi2dOiWFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dpo = TrainDPO(data, dpo_config)\n",
        "train_dpo.train_and_save_model()"
      ],
      "metadata": {
        "id": "GwU9GLPgi8Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing"
      ],
      "metadata": {
        "id": "VVKAbRW8kd7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM"
      ],
      "metadata": {
        "id": "w2yUa5zWjB8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoPeftModelForCausalLM.from_pretrained(\"snshrivas10/tiny-chatbot-dpo\")"
      ],
      "metadata": {
        "id": "RCbtrKJnkjPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "QpVFzZbFk57w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "_yoIWMhBlEkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "kLVE3LCUlJId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pipeline(\"text-generation\",model=model,tokenizer=\"snshrivas10/tiny-chatbot-dpo\",torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "8wxdpJlTlMKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "QN3MQzQlliC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=pipe.tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)"
      ],
      "metadata": {
        "id": "X0WSgd71lh5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "S9ROHi8OmJet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)"
      ],
      "metadata": {
        "id": "lrqYl_d0mPQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "6OVDI1e1mYbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Discuss the causes of the Great Depression\"},\n",
        "]"
      ],
      "metadata": {
        "id": "NpY57D9PojlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2CtOI29mxws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
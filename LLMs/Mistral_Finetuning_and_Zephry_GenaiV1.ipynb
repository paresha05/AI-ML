{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxblls78SChu"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "7E5fOOjlSWJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "WIFpEbEfSYHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl py7zr auto-gptq optimum"
      ],
      "metadata": {
        "id": "sGOJxE5GSbIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "WqrcqU7nS5xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "AfN1ChCYTDyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "igCU-a85TWFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "SDFcuObMTZiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset,Dataset"
      ],
      "metadata": {
        "id": "6pjBpmchTxhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=load_dataset(\"samsum\",split=\"train\")"
      ],
      "metadata": {
        "id": "dialMtCuT24v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "qM7IqlccdqoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "wN91o6rTedaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "gZeAwhykd8xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.sample(50)"
      ],
      "metadata": {
        "id": "cH-jVah6dyio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eRuc_Idd3f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf=df.to_pandas()"
      ],
      "metadata": {
        "id": "6vjRPfHZULsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf.head()"
      ],
      "metadata": {
        "id": "w7Tt0an0UQf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf=datadf.sample(50)"
      ],
      "metadata": {
        "id": "xKXCjZAtekSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf[\"text\"]=datadf[[\"dialogue\", \"summary\"]].apply(lambda x: \"###Human: Summarize this following dialogue: \" + x[\"dialogue\"] + \"\\n###Assistant: \" +x[\"summary\"], axis=1 )"
      ],
      "metadata": {
        "id": "tczgVo_0UUAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf.head()"
      ],
      "metadata": {
        "id": "xZyqW6s6UrQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf[\"text\"].iloc[0]"
      ],
      "metadata": {
        "id": "g3GK0kX3U6Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadf.head(1)"
      ],
      "metadata": {
        "id": "ViDo0PpnZ3zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=Dataset.from_pandas(datadf)"
      ],
      "metadata": {
        "id": "IUB-eOdjZ6Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments"
      ],
      "metadata": {
        "id": "To4LT28-U98A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")"
      ],
      "metadata": {
        "id": "Kw9nXObgV3Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "Piu_tgpkWCuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token=tokenizer.eos_token"
      ],
      "metadata": {
        "id": "7M-_prwSWetn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "c4pMKpY7WiX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModelForCausalLM.from_pretrained( \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
        "                                     quantization_config=quantization_config_loading,\n",
        "                                      device_map=\"auto\")"
      ],
      "metadata": {
        "id": "BNgvniPGWuKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "nGYdkvckXKVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache=False\n",
        "model.config.pretraining_tp=1\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "i1X1T_FbXlty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "BUuluUpCXy66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "NN-wEEadX-Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig"
      ],
      "metadata": {
        "id": "d5ico8F-YBAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config=LoraConfig(\n",
        "    r=16,lora_alpha=16,lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules= [\"q_proj\", \"v_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Kbr3biKhYHS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model"
      ],
      "metadata": {
        "id": "T22haumHYvuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=get_peft_model(model,peft_config)"
      ],
      "metadata": {
        "id": "QnFGNBKpY8f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "W5txlWvFZCEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "5Bb4JosaZc6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"mistral-finetuned-samsum\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=1,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=250,\n",
        "        fp16=True,\n",
        "        push_to_hub=True\n",
        "    )"
      ],
      "metadata": {
        "id": "JlUwuHtUZMBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "Ug6P6soha1h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "jbsLdRM_a1_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    args=training_arguments,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False,\n",
        "    max_seq_length=512\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "RdgIIRtuZnz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "j4GuxW99akk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "99c8GMxfa-p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r /content/mistral-finetuned-samsum /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "o9qFbEqKfVls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# infercening"
      ],
      "metadata": {
        "id": "DWCHsmprgY0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import GenerationConfig\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "KS0xXBCNfVpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/mistral-finetuned-samsum\")"
      ],
      "metadata": {
        "id": "NQwjwQbyf0mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"\"\"\n",
        "###Human: Summarize this following dialogue: Vasanth: I'm at the railway station in Chennai Karthik: No problems so far? Vasanth: no, everything's going smoothly Karthik: good. lets meet there soon!\n",
        "###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "DJeM1979gFFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"/content/mistral-finetuned-samsum\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\")"
      ],
      "metadata": {
        "id": "7homeKh1fyt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=25,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "2BkqygNMf7Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "st_time = time.time()\n",
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(time.time()-st_time)"
      ],
      "metadata": {
        "id": "mvj2IQhUf_05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zephyr finetuning"
      ],
      "metadata": {
        "id": "aGkpikzmhpxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "dOdIJ9jdX4jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets transformers trl peft accelerate bitsandbytes auto-gptq optimum"
      ],
      "metadata": {
        "id": "sUIzgRR3hkhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments\n",
        "from trl import SFTTrainer\n"
      ],
      "metadata": {
        "id": "3hZhLRJXb4bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    MODEL_ID = \"TheBloke/zephyr-7B-alpha-GPTQ\"\n",
        "    DATASET_ID = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\"\n",
        "    CONTEXT_FIELD= \"\"\n",
        "    INSTRUCTION_FIELD = \"instruction\"\n",
        "    TARGET_FIELD = \"response\"\n",
        "    BITS = 4\n",
        "    DISABLE_EXLLAMA = True\n",
        "    DEVICE_MAP = \"auto\"\n",
        "    USE_CACHE = False\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "    BIAS = \"none\"\n",
        "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
        "    TASK_TYPE = \"CAUSAL_LM\"\n",
        "    OUTPUT_DIR = \"zephyr-support-chatbot\"\n",
        "    BATCH_SIZE = 8\n",
        "    GRAD_ACCUMULATION_STEPS = 1\n",
        "    OPTIMIZER = \"paged_adamw_32bit\"\n",
        "    LR = 2e-4\n",
        "    LR_SCHEDULER = \"cosine\"\n",
        "    LOGGING_STEPS = 50\n",
        "    SAVE_STRATEGY = \"epoch\"\n",
        "    NUM_TRAIN_EPOCHS = 1\n",
        "    MAX_STEPS = 250\n",
        "    FP16 = True\n",
        "    PUSH_TO_HUB = True\n",
        "    DATASET_TEXT_FIELD = \"text\"\n",
        "    MAX_SEQ_LENGTH = 512\n",
        "    PACKING = False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k5y2axdrcH0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZephyrTrainer:\n",
        "  def __init__(self):\n",
        "    self.config = Config()\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_ID)\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "\n",
        "  def process_data_sample(self,example):\n",
        "        processed_example = \"<|system|>\\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\\n<|user|>\\n\" + example[self.config.INSTRUCTION_FIELD] + \"\\n<|assistant|>\\n\" + example[self.config.TARGET_FIELD]\n",
        "\n",
        "        return processed_example\n",
        "\n",
        "\n",
        "  def create_dataset(self):\n",
        "        data = load_dataset(self.config.DATASET_ID, split=\"train\")\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tDOWNLOADED DATASET\")\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        df = data.to_pandas()\n",
        "        df[self.config.DATASET_TEXT_FIELD] = df[[self.config.INSTRUCTION_FIELD, self.config.TARGET_FIELD]].apply(lambda x: self.process_data_sample(x), axis=1)\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tPROCESSED DATASET\")\n",
        "        print(df.iloc[0])\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        processed_data = Dataset.from_pandas(df[[self.config.DATASET_TEXT_FIELD]])\n",
        "        return processed_data\n",
        "\n",
        "\n",
        "\n",
        "  def prepare_model(self):\n",
        "        bnb_config = GPTQConfig(\n",
        "                                    bits=self.config.BITS,\n",
        "                                    disable_exllama=self.config.DISABLE_EXLLAMA,\n",
        "                                    tokenizer=self.tokenizer\n",
        "                                )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "                                                        self.config.MODEL_ID,\n",
        "                                                        quantization_config=bnb_config,\n",
        "                                                        device_map=self.config.DEVICE_MAP\n",
        "                                                    )\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tDOWNLOADED MODEL\")\n",
        "        print(model)\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        model.config.use_cache=self.config.USE_CACHE\n",
        "        model.config.pretraining_tp=1\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tMODEL CONFIG UPDATED\")\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        peft_config = LoraConfig(\n",
        "                                    r=self.config.LORA_R,\n",
        "                                    lora_alpha=self.config.LORA_ALPHA,\n",
        "                                    lora_dropout=self.config.LORA_DROPOUT,\n",
        "                                    bias=self.config.BIAS,\n",
        "                                    task_type=self.config.TASK_TYPE,\n",
        "                                    target_modules=self.config.TARGET_MODULES\n",
        "                                )\n",
        "\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tPREPARED MODEL FOR FINETUNING\")\n",
        "        print(model)\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        return model, peft_config\n",
        "  def set_training_arguments(self):\n",
        "        training_arguments = TrainingArguments(\n",
        "                                                output_dir=self.config.OUTPUT_DIR,\n",
        "                                                per_device_train_batch_size=self.config.BATCH_SIZE,\n",
        "                                                gradient_accumulation_steps=self.config.GRAD_ACCUMULATION_STEPS,\n",
        "                                                optim=self.config.OPTIMIZER,\n",
        "                                                learning_rate=self.config.LR,\n",
        "                                                lr_scheduler_type=self.config.LR_SCHEDULER,\n",
        "                                                save_strategy=self.config.SAVE_STRATEGY,\n",
        "                                                logging_steps=self.config.LOGGING_STEPS,\n",
        "                                                num_train_epochs=self.config.NUM_TRAIN_EPOCHS,\n",
        "                                                max_steps=self.config.MAX_STEPS,\n",
        "                                                fp16=self.config.FP16,\n",
        "                                                push_to_hub=self.config.PUSH_TO_HUB\n",
        "                                            )\n",
        "\n",
        "        return training_arguments\n",
        "  def train(self):\n",
        "    data=self.create_dataset()\n",
        "    model,peft_config=self.prepare_model()\n",
        "    training_args=self.set_training_arguments()\n",
        "\n",
        "    trainer=SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=data,\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=self.config.DATASET_TEXT_FIELD,\n",
        "        args=training_args,\n",
        "        tokenizer=self.tokenizer,\n",
        "        packing=self.config.PACKING,\n",
        "        max_seq_length=self.config.MAX_SEQ_LENGTH\n",
        "\n",
        "    )\n",
        "    trainer.train()\n",
        "    print(\"\\n====================================================================\\n\")\n",
        "    print(\"\\t\\t\\tFINETUNING COMPLETED\")\n",
        "    print(\"\\n====================================================================\\n\")\n",
        "\n",
        "    trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "waK9wO-JcuI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zephyr_trainer = ZephyrTrainer()"
      ],
      "metadata": {
        "id": "nTRYkpnqiWWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zephyr_trainer.train()"
      ],
      "metadata": {
        "id": "fuHwCZNdi26q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import GenerationConfig\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "3c4stimSsRZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_sample(example):\n",
        "\n",
        "    processed_example = \"<|system|>\\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\\n<|user|>\\n\" + example[\"instruction\"] + \"\\n<|assistant|>\\n\"\n",
        "\n",
        "    return processed_example\n"
      ],
      "metadata": {
        "id": "9bzeJs6usUWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/zephyr-support-chatbot\")\n"
      ],
      "metadata": {
        "id": "CjXDUh78sZtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_str = process_data_sample(\n",
        "    {\n",
        "        \"instruction\": \"i have a question about cancelling order {{Order Number}}\",\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "o1O9s_24soV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(inp_str, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "5OREOdeqsqbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"/content/zephyr-support-chatbot\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\")\n"
      ],
      "metadata": {
        "id": "AMHM3YNPss5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n"
      ],
      "metadata": {
        "id": "g8Nr4lVCjOs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "st_time = time.time()\n",
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(time.time()-st_time)"
      ],
      "metadata": {
        "id": "-WSINJA0jaSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2s-dYdV8s6Pf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}